项目：CaseSentinel（案件哨兵） - LLM原生主动式刑侦策略Agent系统
1. 项目愿景与核心目标
项目愿景: 研发一个具备认知推理与自主规划能力的AI智能参谋系统“CaseSentinel（案件哨兵）”，旨在通过模拟人类顶级侦探的思维方式，主动生成并迭代最优侦查策略，赋能刑事侦查人员。

核心目标: 实现从“数据驱动”到**“推理驱动”的范式转变。系统应能以自然语言理解案情，在不确定性中进行链式思考 (Chain-of-Thought)**，并自主规划、调用工具以达成“破案”这一最终目标。

理论基石: 本项目将刑侦视为一个动态的、基于LLM的信念管理与探索过程。系统的核心是一个由LLM维护的、富文本格式的**“中心思想板 (Central Blackboard)”**，它取代了僵化的假说空间图（HSG），允许Agent以更灵活的方式进行协作推理。

**“假说空间图 (HSG)”**作为系统的核心理论模型。

1.1 模型定义: HSG是一个动态的、概率性的图结构，用以在数学上表征一个案件的全部认知状态。

节点 (Node): 图中的每个节点 H i
​代表一个具体的、可被证伪的犯罪假说。例如：

H 1: “张三因情仇杀害李四”。

H 2: “王五为财图财杀害李四”。

H 3: “李四系意外死亡”。

节点属性: 每个假说节点 H i的核心属性是其置信度 P(H i∣E)，即在当前所有证据 E 的条件下，该假说为真的后验概率。

边 (Edge): 边代表假说之间的逻辑关系，如“互斥”、“包含”或“因果关联”。

1.2 侦查目标的重新定义:
在HSG模型下，侦查的最终目标不再是找到某个“关键证据”，而是驱动整个假说空间图的概率分布达到收敛。理想的侦破状态是：

某个假说 H ∗  的置信度 P(H ∗ ∣E) 趋近于1。

所有其他竞争假说的置信度趋近于0。
侦查的过程目标，则是通过一系列侦查行动 (Action)，最高效地改变HSG的概率分布，即最大化信息增益。

1.3 理论创新:
这个模型将刑侦问题从一个“规划问题”成功转化为了一个更深刻、更前沿的**“动态贝叶斯推理与最优实验设计 (Optimal Experimental Design)”**问题。这不仅更符合刑侦工作的本质，也为我们的系统提供了坚实的理论基础和创新高度。

2. 项目实施路线图：四大阶段
阶段一：奠基与知识赋能 (Phase 1: Foundation & Knowledge Empowerment)
目标: 为LLM Agent提供高质量的“燃料”——结构化知识和可调用的工具。

步骤 1.1: 中心思想板模式定义 (Blackboard Schema)

任务: 定义系统共享工作区的核心数据结构，它将以富文本（Markdown）格式存在。

产出:

blackboard_template.md: 定义“中心思想板”结构的Markdown模板，包含 # 案件概述, # 现有证据列表, # 核心假说, # 待办侦查行动 等章节。

tools_spec.py: 定义Agent可调用的工具集，如 search_internal_database() 和 run_facial_recognition()。

步骤 1.2: 数据处理流水线（为LLM优化）

任务: 将原始裁判文书转化为可供LLM微调或上下文学习的高质量文本样本。

产出:

cleaner.py: 负责清洗文书。

narrative_generator.py: 调用强大LLM，将案情改写成信息密度高的**“侦探小说”式叙事文本**，以激发LLM的推理能力。

fine_tuning_generator.py: 生成用于微调核心Agent的**“思考过程”样本**，格式如 (案情描述, 侦探的推理链, 最终决策)。

步骤 1.3: 领域知识库

任务: 为Agent提供进行RAG（检索增强生成）所需的外部专业知识。

产出: 向量数据库及用于导入知识的 importer.py 脚本。

阶段二：LLM Agent的实例化与赋能 (Phase 2: Agent Instantiation & Empowerment)
目标: 开发系统的“认知核心”，每个Agent都是由LLM驱动的、拥有特定“人设”和工具集的自主实体。

步骤 2.1: 核心Agent的定义与微调

任务: 定义每个Agent的LLM核心，并进行专业化训练。

产出:

“分析师”Agent (Analyst Agent):

人设: 严谨、注重细节。

核心能力: 读取“中心思想板”，进行多轮CoT推理，输出对所有假说的重新评估，并更新思想板上的# 核心假说章节。

“战略家”Agent (Strategist Agent):

人设: 经验丰富、高瞻远瞩。

核心能力: 基于微调后的策略知识生成下一步行动建议，并输出支持决策的战略考量。

“预测员”Agent (Forecaster Agent):

人设: 冷静、数据驱动。

核心能力: 作为一个微调过的专用LLM，接收行动方案，输出结果的概率分布，并附上**“为什么这么预测”的文字解释**。

步骤 2.2: Agent执行框架

任务: 搭建框架（如使用LangChain或自研）来管理Agent生命周期、工具调用和与“中心思想板”的交互。

产出: agent_framework.py 脚本。

阶段三：系统集成与协作推理 (Phase 3: Integration & Collaborative Reasoning)
目标: 让独立的Agent在一个共享的工作空间中进行真正的协作。

步骤 3.1: 主控制循环（基于思想板）

任务: 编排Agent的协作流程。

产出: main.py，实现一个包含分析、战略、预测与决策、人类交互、更新思想板等阶段的循环。

步骤 3.2: 可视化界面（升级）

任务: 不仅可视化数据，更要可视化“思考过程”。

产出: 一个Web界面，能实时展示每个Agent的“内心独白”（即其CoT推理链）。

阶段四：评估、优化与论文撰写 (Phase 4: Evaluation, Refinement & Paper Writing)
目标: 评估系统的推理能力，而不仅仅是任务完成的效率。

步骤 4.1: 评估框架（升级）

任务: 建立一套能评估Agent推理质量的流程。

产出: evaluation.py，新增推理链合理性评估和鲁棒性测试（如加入误导性证据）。

步骤 4.2: 对比实验与迭代（新基线）

任务: 证明LLM-Native架构的优越性。

产出: 实验结果报告，与以下基线进行对比：

单一通用LLM: 用单个大Prompt让GPT-4o解决问题。

规则+机器学习系统: 一个非LLM驱动的传统AI版本。

消融研究: 去掉某个Agent，观察系统性能的下降。